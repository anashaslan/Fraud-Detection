# -*- coding: utf-8 -*-
"""Fraud detection cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G4blITNlgGEz_CXYgeV1HE4DWcGtn7PP
"""

from google.colab import drive
import os
import zipfile

from google.colab import drive
drive.mount('/content/drive')

!gdown --id 17oMWhIzjzTD3GCA3zV8nSTX9LMZugtAK

from google.colab import drive

drive.mount('/content/drive') # Use drive.mount instead of sample_data.mount

dataset_zip = '/content/drive/MyDrive/Data/archive (1).zip'  # Update the path if using Google Drive
with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:
    zip_ref.extractall('/content/dataset')

base_dir = '/content/dataset/Insurance-Fraud-Detection'  # Dataset location
os.listdir(base_dir)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Step 3: Dataset Preparation
base_dir = '/content/dataset/Insurance-Fraud-Detection/Insurance-Fraud-Detection'  # Dataset location
train_dir = os.path.join(base_dir, 'train')
test_dir = os.path.join(base_dir, 'test')

import os # Import the os module
fraud_dir = os.path.join(train_dir, 'Fraud')  # Fraudulent claims directory
non_fraud_dir = os.path.join(train_dir, 'Non-Fraud')  # Non-fraudulent claims directory

# Step 3.1: Oversampling Fraudulent Claims
fraud_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

fraud_augmented_dir = os.path.join('/content/dataset/train/augmented_fraud')
os.makedirs(fraud_augmented_dir, exist_ok=True)

fraud_images = os.listdir(fraud_dir)
fraud_augmented_images = []

for img_name in fraud_images:
    img_path = os.path.join(fraud_dir, img_name)
    img = plt.imread(img_path)
    img = np.expand_dims(img, axis=0)  # Add batch dimension
    augmented = fraud_datagen.flow(img, batch_size=1)
    for i in range(25):  # Generate 25 augmented samples per original image
        augmented_img = augmented.__next__()[0] # Use __next__() instead of next()
        fraud_augmented_images.append(augmented_img)
        augmented_img_path = os.path.join(fraud_augmented_dir, f"augmented_{img_name.split('.')[0]}_{i}.jpg")
        plt.imsave(augmented_img_path, augmented_img)

import os # Import the os module

# Step 3.2: Combine Fraud and Non-Fraud Data
combined_train_dir = '/content/dataset/train_combined'
os.makedirs(combined_train_dir, exist_ok=True)

os.system(f"cp -r {non_fraud_dir} {combined_train_dir}/non_fraud")
os.system(f"cp -r {fraud_dir} {combined_train_dir}/fraud")
os.system(f"cp -r {fraud_augmented_dir}/* {combined_train_dir}/fraud")

# Step 3.3: Image Data Generators for Training and Testing
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

test_datagen = ImageDataGenerator(rescale=1.0/255)

train_generator = train_datagen.flow_from_directory(
    combined_train_dir,
    target_size=(128, 128),
    batch_size=32,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(128, 128),
    batch_size=32,
    class_mode='binary'
)

# Step 4: Build CNN Model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Step 5: Train the Model
history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=10,
    validation_data=test_generator,
    validation_steps=len(test_generator)
)

loss, accuracy = model.evaluate(test_generator)
print(f"Test Accuracy: {accuracy*100:.2f}%")

# Step 7: Visualize Training Results
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Step 1: Get predictions from the CNN model
y_true = test_generator.classes  # Ground truth labels
y_pred_prob = model.predict(test_generator)  # Predicted probabilities
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix # Import necessary functions

# Step 2: Calculate metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
conf_matrix = confusion_matrix(y_true, y_pred)

# Step 3: Print metrics
from sklearn.metrics import classification_report # Import classification_report
print("Evaluation Metrics for CNN:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))

# Step 4: Visualize confusion matrix
!pip install seaborn  # Install seaborn if not already installed

import seaborn as sns  # Import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()